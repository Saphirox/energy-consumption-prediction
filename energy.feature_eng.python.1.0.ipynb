{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%% # <center> Feature engineering\n"
    }
   },
   "source": [],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r __ipy\n",
    "%store -r __da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper ipython script loaded\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    body {\n",
       "          font-family: Helvetica, Times New Roman, sans-serif;\n",
       "    }\n",
       "    \n",
       "    h1,h2, h3,h4,h5,h6 {\n",
       "        font-family: Rockwell, Times New Roman, sans-serif;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "__ipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Data Analysis tools was loaded\n"
     ]
    }
   ],
   "source": [
    "__da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = 'data/'\n",
    "\n",
    "train_df = pd.read_pickle(root + 'train_df.pkl')\n",
    "test_df = pd.read_pickle(root + 'test_df.pkl')\n",
    "\n",
    "train_df = train_df.query('not (building_id <= 104 & meter == 0 & timestamp <= \"2016-05-20\")')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['age'] = train_df['year_built'].max() - train_df['year_built'] + 1\n",
    "test_df['age'] = test_df['year_built'].max() - test_df['year_built'] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import math\n",
    "\n",
    "le = LabelEncoder()\n",
    "train_df['primary_use'] = train_df['primary_use'].astype(str)\n",
    "train_df['primary_use'] = le.fit_transform(train_df['primary_use']).astype(np.int8)\n",
    "\n",
    "test_df['primary_use'] = test_df['primary_use'].astype(str)\n",
    "test_df['primary_use'] = le.fit_transform(test_df['primary_use']).astype(np.int8)\n",
    "\n",
    "train_df['floor_count'] = train_df['floor_count'].fillna(-999).astype(np.int16)\n",
    "test_df['floor_count'] = test_df['floor_count'].fillna(-999).astype(np.int16)\n",
    "\n",
    "train_df['year_built'] = train_df['year_built'].fillna(-999).astype(np.int16)\n",
    "test_df['year_built'] = test_df['year_built'].fillna(-999).astype(np.int16)\n",
    "\n",
    "train_df['age'] = train_df['age'].fillna(-999).astype(np.int16)\n",
    "test_df['age'] = test_df['age'].fillna(-999).astype(np.int16)\n",
    "\n",
    "train_df['cloud_coverage'] = train_df['cloud_coverage'].fillna(-999).astype(np.int16)\n",
    "test_df['cloud_coverage'] = test_df['cloud_coverage'].fillna(-999).astype(np.int16) \n",
    "\n",
    "\n",
    "train_df['month_datetime'] = train_df['timestamp'].dt.month.astype(np.int8)\n",
    "train_df['weekofyear_datetime'] = train_df['timestamp'].dt.weekofyear.astype(np.int8)\n",
    "train_df['dayofyear_datetime'] = train_df['timestamp'].dt.dayofyear.astype(np.int16)\n",
    "    \n",
    "train_df['hour_datetime'] = train_df['timestamp'].dt.hour.astype(np.int8)  \n",
    "train_df['day_week'] = train_df['timestamp'].dt.dayofweek.astype(np.int8)\n",
    "train_df['day_month_datetime'] = train_df['timestamp'].dt.day.astype(np.int8)\n",
    "train_df['week_month_datetime'] = train_df['timestamp'].dt.day/7\n",
    "train_df['week_month_datetime'] = train_df['week_month_datetime'].apply(lambda x: math.ceil(x)).astype(np.int8)\n",
    "    \n",
    "    \n",
    "test_df['month_datetime'] = test_df['timestamp'].dt.month.astype(np.int8)\n",
    "test_df['weekofyear_datetime'] = test_df['timestamp'].dt.weekofyear.astype(np.int8)\n",
    "test_df['dayofyear_datetime'] = test_df['timestamp'].dt.dayofyear.astype(np.int16)\n",
    "    \n",
    "test_df['hour_datetime'] = test_df['timestamp'].dt.hour.astype(np.int8)\n",
    "test_df['day_week'] = test_df['timestamp'].dt.dayofweek.astype(np.int8)\n",
    "test_df['day_month_datetime'] = test_df['timestamp'].dt.day.astype(np.int8)\n",
    "test_df['week_month_datetime'] = test_df['timestamp'].dt.day/7\n",
    "test_df['week_month_datetime'] = test_df['week_month_datetime'].apply(lambda x: math.ceil(x)).astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['building_id', 'meter', 'timestamp', 'meter_reading', 'site_id',\n",
       "       'primary_use', 'square_feet', 'year_built', 'floor_count',\n",
       "       'air_temperature', 'cloud_coverage', 'dew_temperature',\n",
       "       'precip_depth_1_hr', 'sea_level_pressure', 'wind_direction',\n",
       "       'wind_speed', 'age', 'month_datetime', 'weekofyear_datetime',\n",
       "       'dayofyear_datetime', 'hour_datetime', 'day_week', 'day_month_datetime',\n",
       "       'week_month_datetime', 'meter_reading_log'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d766f04714a34a99a1467e5c0c660d35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20216100), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "KEY = ['building_id', 'meter', 'site_id']\n",
    "\n",
    "def to_key(train_df):\n",
    "    le_key = LabelEncoder()\n",
    "    train_df['building_id__meter__key'] = train_df[KEY].progress_apply(\\\n",
    "        lambda x: str(x['building_id']) + \"_\" + str(x['meter']) + \"_\" +   str(x['site_id']), axis=1)\n",
    "    \n",
    "    le_key.fit(train_df['building_id__meter__key'])\n",
    "    train_df['building_id__meter__key'] = le_key.transform(train_df['building_id__meter__key'])\n",
    "    train_df['building_id__meter__key'] = train_df['building_id__meter__key'].astype(np.int32)\n",
    "    return train_df\n",
    "\n",
    "train_df = to_key(train_df)\n",
    "#test_df = to_key(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['meter_reading_log1p'] = np.log1p(train_df['meter_reading'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TSFresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings={\n",
    "'standard_deviation': None,\n",
    " 'variance': None,\n",
    " 'skewness': None,\n",
    " 'kurtosis': None,\n",
    " 'absolute_sum_of_changes': None,\n",
    "\n",
    "'last_location_of_maximum': None,\n",
    " 'first_location_of_maximum': None,\n",
    " 'last_location_of_minimum': None,\n",
    "'first_location_of_minimum': None,\n",
    "'count_above_mean': None,\n",
    " 'count_below_mean': None,\n",
    " 'maximum': None,\n",
    " 'minimum': None,\n",
    "\n",
    " 'c3': [{'lag': 100}, {'lag': 2000}, {'lag': 3000},{'lag': 10000}],\n",
    "\n",
    "'number_peaks': [{'n': 1}, {'n': 5},{'n': 100},{'n': 1000}],\n",
    "'fft_coefficient': \n",
    " [{'coeff': 0, 'attr': 'real'},\n",
    "  {'coeff': 1, 'attr': 'real'},\n",
    "  {'coeff': 2, 'attr': 'real'},\n",
    "  {'coeff': 3, 'attr': 'real'},\n",
    "  {'coeff': 4, 'attr': 'real'},\n",
    "\n",
    "  {'coeff': 0, 'attr': 'imag'},\n",
    "  {'coeff': 1, 'attr': 'imag'},\n",
    "  {'coeff': 2, 'attr': 'imag'},\n",
    "  {'coeff': 3, 'attr': 'imag'},\n",
    "  {'coeff': 4, 'attr': 'imag'},\n",
    "  {'coeff': 5, 'attr': 'imag'},\n",
    "\n",
    "  {'coeff': 0, 'attr': 'abs'},\n",
    "  {'coeff': 1, 'attr': 'abs'},\n",
    "  {'coeff': 2, 'attr': 'abs'},\n",
    "  {'coeff': 3, 'attr': 'abs'},\n",
    "  {'coeff': 4, 'attr': 'abs'},\n",
    "  {'coeff': 5, 'attr': 'abs'},\n",
    "\n",
    "  {'coeff': 0, 'attr': 'angle'},\n",
    "  {'coeff': 1, 'attr': 'angle'},\n",
    "  {'coeff': 2, 'attr': 'angle'},\n",
    "  {'coeff': 3, 'attr': 'angle'},\n",
    "  {'coeff': 4, 'attr': 'angle'},\n",
    "  {'coeff': 5, 'attr': 'angle'},\n",
    "],\n",
    " 'agg_linear_trend': [\n",
    "     {'attr': 'rvalue', 'chunk_len': 500, 'f_agg': 'max'},\n",
    "  {'attr': 'rvalue', 'chunk_len': 500, 'f_agg': 'min'},\n",
    "  {'attr': 'rvalue', 'chunk_len': 500, 'f_agg': 'mean'},\n",
    "  {'attr': 'rvalue', 'chunk_len': 500, 'f_agg': 'var'},\n",
    "  {'attr': 'rvalue', 'chunk_len': 1000, 'f_agg': 'max'},\n",
    "  {'attr': 'rvalue', 'chunk_len': 1000, 'f_agg': 'min'},\n",
    "  {'attr': 'rvalue', 'chunk_len': 1000, 'f_agg': 'mean'},\n",
    "  {'attr': 'rvalue', 'chunk_len': 1000, 'f_agg': 'var'},\n",
    "  {'attr': 'rvalue', 'chunk_len': 5000, 'f_agg': 'max'},\n",
    "  {'attr': 'rvalue', 'chunk_len': 5000, 'f_agg': 'min'},\n",
    "  {'attr': 'rvalue', 'chunk_len': 5000, 'f_agg': 'mean'},\n",
    "  {'attr': 'rvalue', 'chunk_len': 5000, 'f_agg': 'var'},\n",
    "  {'attr': 'intercept', 'chunk_len': 500, 'f_agg': 'max'},\n",
    "  {'attr': 'intercept', 'chunk_len': 500, 'f_agg': 'min'},\n",
    "  {'attr': 'intercept', 'chunk_len': 500, 'f_agg': 'mean'},\n",
    "  {'attr': 'intercept', 'chunk_len': 500, 'f_agg': 'var'},\n",
    "  {'attr': 'intercept', 'chunk_len': 1000, 'f_agg': 'max'},\n",
    "  {'attr': 'intercept', 'chunk_len': 1000, 'f_agg': 'min'},\n",
    "  {'attr': 'intercept', 'chunk_len': 1000, 'f_agg': 'mean'},\n",
    "  {'attr': 'intercept', 'chunk_len': 1000, 'f_agg': 'var'},\n",
    "  {'attr': 'intercept', 'chunk_len': 5000, 'f_agg': 'max'},\n",
    "  {'attr': 'intercept', 'chunk_len': 5000, 'f_agg': 'min'},\n",
    "  {'attr': 'intercept', 'chunk_len': 5000, 'f_agg': 'mean'},\n",
    "  {'attr': 'intercept', 'chunk_len': 5000, 'f_agg': 'var'},\n",
    "  {'attr': 'slope', 'chunk_len': 500, 'f_agg': 'max'},\n",
    "  {'attr': 'slope', 'chunk_len': 500, 'f_agg': 'min'},\n",
    "  {'attr': 'slope', 'chunk_len': 500, 'f_agg': 'mean'},\n",
    "  {'attr': 'slope', 'chunk_len': 500, 'f_agg': 'var'},\n",
    "  {'attr': 'slope', 'chunk_len': 1000, 'f_agg': 'max'},\n",
    "  {'attr': 'slope', 'chunk_len': 1000, 'f_agg': 'min'},\n",
    "  {'attr': 'slope', 'chunk_len': 1000, 'f_agg': 'mean'},\n",
    "  {'attr': 'slope', 'chunk_len': 1000, 'f_agg': 'var'},\n",
    "  {'attr': 'slope', 'chunk_len': 5000, 'f_agg': 'max'},\n",
    "  {'attr': 'slope', 'chunk_len': 5000, 'f_agg': 'min'},\n",
    "  {'attr': 'slope', 'chunk_len': 5000, 'f_agg': 'mean'},\n",
    "  {'attr': 'slope', 'chunk_len': 5000, 'f_agg': 'var'},\n",
    "  {'attr': 'stderr', 'chunk_len': 500, 'f_agg': 'max'},\n",
    "  {'attr': 'stderr', 'chunk_len': 500, 'f_agg': 'min'},\n",
    "  {'attr': 'stderr', 'chunk_len': 500, 'f_agg': 'mean'},\n",
    "  {'attr': 'stderr', 'chunk_len': 500, 'f_agg': 'var'},\n",
    "  {'attr': 'stderr', 'chunk_len': 1000, 'f_agg': 'max'},\n",
    "  {'attr': 'stderr', 'chunk_len': 1000, 'f_agg': 'min'},\n",
    "  {'attr': 'stderr', 'chunk_len': 1000, 'f_agg': 'mean'},\n",
    "  {'attr': 'stderr', 'chunk_len': 1000, 'f_agg': 'var'},\n",
    "  {'attr': 'stderr', 'chunk_len': 5000, 'f_agg': 'max'},\n",
    "  {'attr': 'stderr', 'chunk_len': 5000, 'f_agg': 'min'},\n",
    "  {'attr': 'stderr', 'chunk_len': 5000, 'f_agg': 'mean'},\n",
    "  {'attr': 'stderr', 'chunk_len': 5000, 'f_agg': 'var'}],\n",
    " 'index_mass_quantile': [{'q': 0.1},\n",
    "  {'q': 0.2},\n",
    "  {'q': 0.3},\n",
    "  {'q': 0.4},\n",
    "  {'q': 0.6},\n",
    "  {'q': 0.7},\n",
    "  {'q': 0.8},\n",
    "  {'q': 0.9}],\n",
    "\n",
    " 'spkt_welch_density': [{'coeff': 2}, {'coeff': 5}, {'coeff': 8}],\n",
    " 'ar_coefficient': [{'coeff': 0, 'k': 10},\n",
    "  {'coeff': 1, 'k': 10},\n",
    "  {'coeff': 2, 'k': 10},\n",
    "  {'coeff': 3, 'k': 10},\n",
    "  {'coeff': 4, 'k': 10}],\n",
    "\n",
    " 'energy_ratio_by_chunks': [{'num_segments': 10, 'segment_focus': 0},\n",
    "  {'num_segments': 10, 'segment_focus': 1},\n",
    "  {'num_segments': 10, 'segment_focus': 2},\n",
    "  {'num_segments': 10, 'segment_focus': 3},\n",
    "  {'num_segments': 10, 'segment_focus': 4},\n",
    "  {'num_segments': 10, 'segment_focus': 5},\n",
    "  {'num_segments': 10, 'segment_focus': 6},\n",
    "  {'num_segments': 10, 'segment_focus': 7},\n",
    "  {'num_segments': 10, 'segment_focus': 8},\n",
    "  {'num_segments': 10, 'segment_focus': 9}],\n",
    " 'ratio_beyond_r_sigma': [{'r': 0.5},\n",
    "  {'r': 1},\n",
    "  {'r': 1.5},\n",
    "  {'r': 2},\n",
    "  {'r': 2.5},\n",
    "  {'r': 3},\n",
    "  {'r': 5},\n",
    "  {'r': 6},\n",
    "  {'r': 7},\n",
    "  {'r': 10}],\n",
    "'max_langevin_fixed_point': [{'m': 3, 'r': 30}],\n",
    "    'change_quantiles': [{'ql': 0.0, 'qh': 0.2, 'isabs': False, 'f_agg': 'mean'},\n",
    "  {'ql': 0.0, 'qh': 0.2, 'isabs': False, 'f_agg': 'var'},\n",
    "  {'ql': 0.0, 'qh': 0.2, 'isabs': True, 'f_agg': 'mean'},\n",
    "  {'ql': 0.0, 'qh': 0.2, 'isabs': True, 'f_agg': 'var'},\n",
    "  {'ql': 0.0, 'qh': 0.4, 'isabs': False, 'f_agg': 'mean'},\n",
    "  {'ql': 0.0, 'qh': 0.4, 'isabs': False, 'f_agg': 'var'},\n",
    "  {'ql': 0.0, 'qh': 0.4, 'isabs': True, 'f_agg': 'mean'},\n",
    "  {'ql': 0.0, 'qh': 0.4, 'isabs': True, 'f_agg': 'var'},\n",
    "  {'ql': 0.0, 'qh': 0.6, 'isabs': False, 'f_agg': 'mean'},\n",
    "  {'ql': 0.0, 'qh': 0.6, 'isabs': False, 'f_agg': 'var'},\n",
    "  {'ql': 0.0, 'qh': 0.6, 'isabs': True, 'f_agg': 'mean'},\n",
    "  {'ql': 0.0, 'qh': 0.6, 'isabs': True, 'f_agg': 'var'},\n",
    "  {'ql': 0.0, 'qh': 0.8, 'isabs': False, 'f_agg': 'mean'},\n",
    "  {'ql': 0.0, 'qh': 0.8, 'isabs': False, 'f_agg': 'var'},\n",
    "  {'ql': 0.0, 'qh': 0.8, 'isabs': True, 'f_agg': 'mean'},\n",
    "  {'ql': 0.0, 'qh': 0.8, 'isabs': True, 'f_agg': 'var'},\n",
    "  {'ql': 0.0, 'qh': 1.0, 'isabs': False, 'f_agg': 'mean'}]\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "# TSFRESH\n",
    "from tsfresh.feature_extraction import extract_features\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "from tsfresh.feature_extraction.settings import EfficientFCParameters, MinimalFCParameters\n",
    "\n",
    "class TSFreshTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, column_id, column_sort, column_value, extraction_settings):\n",
    "        self.column_id = column_id\n",
    "        self.column_sort = column_sort\n",
    "        self.column_value = column_value\n",
    "        self.extraction_settings = extraction_settings\n",
    "\n",
    "    def fit(self, train_x, train_y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X_train, y_train=None, **fit_params):\n",
    "        X_features = extract_features(\n",
    "            X_train,\n",
    "            column_id=self.column_id,\n",
    "            column_sort=self.column_sort,\n",
    "            column_value=self.column_value,\n",
    "            default_fc_parameters=self.extraction_settings)\n",
    "\n",
    "        impute(X_features)\n",
    "        return X_features\n",
    "\n",
    "    def get_params(self, **kwargs):\n",
    "        return {'column_id': self.column_id,\n",
    "                'column_sort': self.column_sort,\n",
    "                'column_value': self.column_value,\n",
    "                'extraction_settings': self.extraction_settings}\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        return self.fit(X).transform(X)\n",
    "\n",
    "def tsfresh_apply(key, train, test, date_col):    \n",
    "    def merge(features, df):\n",
    "        return df.merge(features, how='left', left_on=key, right_on='id', \n",
    "                            right_index=False,\n",
    "                            suffixes=('', f'_{date_col}'))\n",
    "    \n",
    "    features = TSFreshTransformer(key, date_col, 'meter_reading', settings) \\\n",
    "            .fit_transform(train, axis=0)\n",
    "    \n",
    "    train = merge(features, train)\n",
    "    test = merge(features, test)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_columns = [\"timestamp\", \"meter_reading\"]\n",
    "drop_columns_ = [\"timestamp\"]\n",
    "train_df.drop(drop_columns_, axis=1, inplace=True)\n",
    "test_df.drop(drop_columns_, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_day = 366\n",
    "train, val = \\\n",
    "    train_df[train_df['dayofyear_datetime'] != predict_day], train_df[train_df['dayofyear_datetime'] == predict_day]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "866167a7f2ac44d0bca6b01106d10806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Feature Extraction:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Feature Extraction:  10%|█         | 1/10 [00:00<00:02,  3.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Feature Extraction:  20%|██        | 2/10 [00:00<00:02,  3.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Feature Extraction:  30%|███       | 3/10 [00:00<00:01,  3.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Feature Extraction:  40%|████      | 4/10 [00:00<00:01,  4.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Feature Extraction:  50%|█████     | 5/10 [00:01<00:01,  4.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Feature Extraction:  60%|██████    | 6/10 [00:01<00:00,  4.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Feature Extraction:  70%|███████   | 7/10 [00:01<00:00,  5.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Feature Extraction:  80%|████████  | 8/10 [00:01<00:00,  4.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Feature Extraction:  90%|█████████ | 9/10 [00:01<00:00,  5.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Feature Extraction: 100%|██████████| 10/10 [00:02<00:00,  4.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Feature Extraction:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Feature Extraction:  10%|█         | 1/10 [00:00<00:03,  2.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Feature Extraction:  20%|██        | 2/10 [00:00<00:02,  3.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Feature Extraction:  30%|███       | 3/10 [00:00<00:02,  3.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Feature Extraction:  40%|████      | 4/10 [00:00<00:01,  3.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Feature Extraction:  50%|█████     | 5/10 [00:01<00:01,  3.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Feature Extraction:  60%|██████    | 6/10 [00:01<00:01,  3.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Feature Extraction:  70%|███████   | 7/10 [00:02<00:01,  2.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Feature Extraction:  80%|████████  | 8/10 [00:02<00:00,  3.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Feature Extraction:  90%|█████████ | 9/10 [00:02<00:00,  3.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Feature Extraction: 100%|██████████| 10/10 [00:02<00:00,  3.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "for col in tqdm(['month_datetime', 'weekofyear_datetime',\n",
    "             'dayofyear_datetime', 'hour_datetime', 'day_week', 'day_month_datetime',\n",
    "             'week_month_datetime']):\n",
    "    train, val = tsfresh_apply('building_id__meter__key', train, val, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_pickle(root+'train.pkl')\n",
    "val.to_pickle(root+'val.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
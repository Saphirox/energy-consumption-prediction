{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import gc\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GroupKFold\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#from catboost import Pool, CatBoostRegressor\n",
    "import lightgbm as lgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 69\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASHRAE3Preprocessor(object):\n",
    "       \n",
    "    @classmethod\n",
    "    def reduce_memory_usage(cls, df):\n",
    "        numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "        for col in df.columns:\n",
    "            col_type = df[col].dtypes\n",
    "            if col_type in numerics:\n",
    "                c_min = df[col].min()\n",
    "                c_max = df[col].max()\n",
    "                if str(col_type)[:3] == 'int':\n",
    "                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                        df[col] = df[col].astype(np.int8)\n",
    "                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                        df[col] = df[col].astype(np.int16)\n",
    "                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                        df[col] = df[col].astype(np.int32)\n",
    "                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                        df[col] = df[col].astype(np.int64)  \n",
    "                else:\n",
    "                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                        df[col] = df[col].astype(np.float16)\n",
    "                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                        df[col] = df[col].astype(np.float32)\n",
    "                    else:\n",
    "                        df[col] = df[col].astype(np.float64)    \n",
    "\n",
    "        return df\n",
    "  \n",
    "    @classmethod\n",
    "    def timestamp_align(cls, weather_df):\n",
    "        weather_key = ['site_id', 'timestamp']\n",
    "\n",
    "        temp_skeleton = weather_df[weather_key + ['air_temperature']].sort_values(by=weather_key).copy()\n",
    "\n",
    "        # calculate ranks of hourly temperatures within date/site_id chunks\n",
    "        temp_skeleton['temp_rank'] = temp_skeleton.groupby(['site_id', temp_skeleton.timestamp.dt.date])['air_temperature'].rank('average')\n",
    "\n",
    "        # create a dataframe of site_ids (0-16) x mean hour rank of temperature within day (0-23)\n",
    "        df_2d = temp_skeleton.groupby(['site_id', temp_skeleton.timestamp.dt.hour])['temp_rank'].mean().unstack(level=1)\n",
    "\n",
    "        # Subtract the columnID of temperature peak by 14, getting the timestamp alignment gap.\n",
    "        site_ids_offsets = pd.Series(df_2d.values.argmax(axis=1) - 14)\n",
    "        site_ids_offsets.index.name = 'site_id'\n",
    "        \n",
    "        weather_df['offset'] = weather_df.site_id.map(site_ids_offsets)\n",
    "        weather_df['timestamp'] = (weather_df['timestamp'] - pd.to_timedelta(weather_df['offset'], unit='H'))\n",
    "        \n",
    "        del temp_skeleton, df_2d, site_ids_offsets\n",
    "        \n",
    "        return weather_df\n",
    "        \n",
    "    @classmethod\n",
    "    def load_dataset(cls, df_path, weather_path, metadata_path):\n",
    "        metadata = pd.read_csv(metadata_path)\n",
    "        \n",
    "        base_df = pd.read_csv(df_path, parse_dates=['timestamp'])\n",
    "        merged_df = pd.merge(base_df, metadata, how=\"left\", on=[\"building_id\"])\n",
    "\n",
    "        weather_df = pd.read_csv(weather_path, parse_dates=['timestamp'])\n",
    "        weather_df = cls.timestamp_align(weather_df)\n",
    "        \n",
    "        df = pd.merge(merged_df, weather_df, how=\"left\", on=[\"site_id\", \"timestamp\"])\n",
    "        \n",
    "        del metadata, base_df, merged_df, weather_df\n",
    "        \n",
    "        # drop all NaN rows which are generated by timestamp alignment\n",
    "        if(cls.train==True):\n",
    "            df = df.loc[~(df['air_temperature'].isnull() & df['cloud_coverage'].isnull() & df['dew_temperature'].isnull() & df['precip_depth_1_hr'].isnull() & \\\n",
    "                          df['sea_level_pressure'].isnull() & df['wind_direction'].isnull() & df['wind_speed'].isnull() & df['offset'].isnull())]\n",
    "            \n",
    "            df = df.query('not (site_id == 0 & meter == 0 & timestamp <= \"2016-05-20\") and not (meter == 2 & building_id == 1099)')\n",
    "        \n",
    "        df['offset'] = np.uint8(df['offset'])\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    @classmethod\n",
    "    def fit(cls, df):\n",
    "        data_ratios =  df.count()/len(df)\n",
    "        cls.avgs = df.loc[:, data_ratios < 1.0].mean()\n",
    "        cls.pu_le = LabelEncoder()\n",
    "        cls.pu_le.fit(df[\"primary_use\"])\n",
    "     \n",
    "    \n",
    "    @classmethod\n",
    "    def average_imputation(cls, df, column_name):\n",
    "        imputation = df.groupby(['timestamp'])[column_name].mean()\n",
    "    \n",
    "        df.loc[df[column_name].isnull(), column_name] = df[df[column_name].isnull()][[column_name]].apply(lambda x: imputation[df['timestamp'][x.index]].values)\n",
    "        \n",
    "        del imputation\n",
    "        return df\n",
    "\n",
    "    @classmethod\n",
    "    def transform(cls, df):\n",
    "        # refill NAN with averages\n",
    "        columns_with_nan = cls.avgs.index.values\n",
    "        for i in range(len(columns_with_nan)):\n",
    "            df = cls.average_imputation(df, columns_with_nan[i])\n",
    "        \n",
    "        df['primary_use'] = np.uint8(cls.pu_le.transform(df['primary_use']))  # encode labels\n",
    "\n",
    "        # expand datetime into its components\n",
    "        df['hour'] = np.uint8(df['timestamp'].dt.hour)\n",
    "        df['day'] = np.uint8(df['timestamp'].dt.day)\n",
    "        df['weekday'] = np.uint8(df['timestamp'].dt.weekday)\n",
    "        df['dayofweek'] = np.uint8(df['timestamp'].dt.dayofweek)\n",
    "        df['month'] = np.uint8(df['timestamp'].dt.month)\n",
    "        df['year'] = np.uint8(df['timestamp'].dt.year-2000)\n",
    "        \n",
    "        # parse and cast columns to a smaller type\n",
    "        df.rename(columns={\"square_feet\": \"log_square_feet\"}, inplace=True)\n",
    "        df['log_square_feet'] = np.float16(np.log(df['log_square_feet']))\n",
    "        df['year_built'] = np.uint8(df['year_built']-1900)\n",
    "        df['floor_count'] = np.uint8(df['floor_count'])\n",
    "        df['wind_direction'] = np.uint16(df['wind_direction'])\n",
    "    \n",
    "        # extract target column\n",
    "        if 'meter_reading' in df.columns:\n",
    "            df['meter_reading'] = np.log1p(df['meter_reading']).astype(np.float32) # comp metric uses log errors \n",
    "            \n",
    "        return df\n",
    "    \n",
    "    @classmethod\n",
    "    def add_features(cls, df):\n",
    "        beaufort_boundaries = [0, 0.3, 1.6, 3.4, 5.5, 8, 10.8, 13.9, 17.2, 20.8, 24.5, 28.5, 33, 10000]\n",
    "        df['wind_speed_beaufort_scale'] = pd.cut(\n",
    "            x=df['wind_speed'], \n",
    "            bins=beaufort_boundaries, \n",
    "            labels=range(len(beaufort_boundaries)-1),\n",
    "            right=False,\n",
    "        ).cat.codes\n",
    "        df['wind_speed_beaufort_scale'] = np.uint8(df['wind_speed_beaufort_scale'])\n",
    "        \n",
    "        #df.loc[(df['primary_use'] == le.transform(['Education'])[0]) & (df['month'] >= 6) & (df['month'] <= 8), 'is_vacation_month'] = np.int8(1)\n",
    "        #df.loc[df['is_vacation_month']!=1, 'is_vacation_month'] = np.int8(0)\n",
    "        \n",
    "        df['group'] = df['month']\n",
    "        df['group'].replace((6, 7, 8), 21, inplace=True)\n",
    "        df['group'].replace((9, 10, 11), 22, inplace=True)\n",
    "        df['group'].replace((3, 4, 5), 23, inplace=True)\n",
    "        df['group'].replace((1, 2, 12), 24, inplace=True)\n",
    "        df['group'].replace((21), 1, inplace=True)\n",
    "        df['group'].replace((22), 2, inplace=True)\n",
    "        df['group'].replace((23), 3, inplace=True)\n",
    "        df['group'].replace((24), 4, inplace=True)\n",
    "        df['group'] = np.uint8(df['group'])\n",
    "        \n",
    "        \n",
    "        # remove redundant columns\n",
    "        for col in df.columns:\n",
    "            if col in ['timestamp', 'row_id']:\n",
    "                del df[col]\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    @classmethod\n",
    "    def get_prepared_dataset(cls, df_path, weather_path, metadata, train):\n",
    "        cls.train = train\n",
    "        \n",
    "        df = cls.load_dataset(df_path, weather_path, metadata)\n",
    "        \n",
    "        start_mem = df.memory_usage().sum() / 1024**2 \n",
    "      \n",
    "        df = cls.reduce_memory_usage(df)\n",
    "        \n",
    "        end_mem = df.memory_usage().sum() / 1024**2\n",
    "        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem)) \n",
    "        \n",
    "        cls.fit(df)\n",
    "        df = cls.transform(df)\n",
    "        df = cls.add_features(df)\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"./data/train.csv\"\n",
    "test_path = \"./data/test.csv\"\n",
    "\n",
    "train_weather_path = \"./data/weather_train.csv\"\n",
    "test_weather_path = \"./data/weather_test.csv\"\n",
    "\n",
    "metadata_path = './data/building_metadata.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(df_path, df_weather_path, metadata_path, train):\n",
    "    return ASHRAE3Preprocessor.get_prepared_dataset(df_path, df_weather_path, metadata_path, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\maksym_suprunenko\\appdata\\local\\continuum\\miniconda3\\lib\\site-packages\\ipykernel_launcher.py:74: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 1036.33 Mb (59.9% reduction)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\maksym_suprunenko\\appdata\\local\\continuum\\miniconda3\\lib\\site-packages\\numpy\\core\\_methods.py:38: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 2187.13 Mb (59.9% reduction)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "195"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = load_dataset(train_path, train_weather_path, metadata_path, train =True)\n",
    "gc.collect()\n",
    "\n",
    "test_df = load_dataset(test_path, test_weather_path, metadata_path, train =False)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSLE(y_true, y_pred, *args, **kwargs):\n",
    "    return np.sqrt(mean_squared_log_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals = [\"site_id\", \"building_id\", \"primary_use\", \"hour\", \"weekday\", 'dayofweek', \"meter\", \"wind_direction\", 'group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "            'boosting_type': 'gbdt',\n",
    "            'objective': 'regression',\n",
    "            'metric': {'rmse'},\n",
    "            'subsample': 0.4,\n",
    "            'learning_rate': 0.15,\n",
    "            'num_leaves': 40,\n",
    "            'feature_fraction': 0.4,\n",
    "            'lambda_l1': 1,  \n",
    "            'lambda_l2': 1\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\maksym_suprunenko\\appdata\\local\\continuum\\miniconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "skf = GroupKFold(n_splits=4)\n",
    "oof = train_df[['meter_reading']]\n",
    "oof['predict'] = 0\n",
    "predictions =  pd.DataFrame()\n",
    "val_rmsle = []\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "features = [col for col in train_df.columns if col not in ['meter_reading', 'year', 'month', 'day', 'timestamp']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting for a fold: 0 | ⏰: 12:19:49\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's rmse: 1.15764\n",
      "[1000]\tvalid_0's rmse: 1.13656\n",
      "[1500]\tvalid_0's rmse: 1.13006\n",
      "[2000]\tvalid_0's rmse: 1.12443\n",
      "[2500]\tvalid_0's rmse: 1.11976\n",
      "[3000]\tvalid_0's rmse: 1.119\n",
      "[3500]\tvalid_0's rmse: 1.11967\n",
      "Early stopping, best iteration is:\n",
      "[3138]\tvalid_0's rmse: 1.11853\n",
      "Predicting for a fold: 0 | started at | ⏰: 12:30:32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\maksym_suprunenko\\appdata\\local\\continuum\\miniconda3\\lib\\site-packages\\pandas\\core\\indexing.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "c:\\users\\maksym_suprunenko\\appdata\\local\\continuum\\miniconda3\\lib\\site-packages\\ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "c:\\users\\maksym_suprunenko\\appdata\\local\\continuum\\miniconda3\\lib\\site-packages\\ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.36295508201330323\n",
      "Feature importance extraction a fold: 0 | started at | ⏰: 12:50:39\n",
      "Starting for a fold: 1 | ⏰: 12:50:42\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's rmse: 1.13531\n",
      "[1000]\tvalid_0's rmse: 1.11044\n",
      "[1500]\tvalid_0's rmse: 1.09565\n",
      "[2000]\tvalid_0's rmse: 1.09067\n",
      "[2500]\tvalid_0's rmse: 1.0851\n",
      "[3000]\tvalid_0's rmse: 1.08169\n",
      "[3500]\tvalid_0's rmse: 1.07967\n",
      "[4000]\tvalid_0's rmse: 1.07845\n",
      "[4500]\tvalid_0's rmse: 1.07603\n",
      "[5000]\tvalid_0's rmse: 1.07671\n",
      "Early stopping, best iteration is:\n",
      "[4531]\tvalid_0's rmse: 1.07586\n",
      "Predicting for a fold: 1 | started at | ⏰: 13:05:11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\maksym_suprunenko\\appdata\\local\\continuum\\miniconda3\\lib\\site-packages\\pandas\\core\\indexing.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "c:\\users\\maksym_suprunenko\\appdata\\local\\continuum\\miniconda3\\lib\\site-packages\\ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "c:\\users\\maksym_suprunenko\\appdata\\local\\continuum\\miniconda3\\lib\\site-packages\\ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.3384262972001087\n",
      "Feature importance extraction a fold: 1 | started at | ⏰: 13:34:59\n",
      "Starting for a fold: 2 | ⏰: 13:35:02\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's rmse: 1.05216\n",
      "[1000]\tvalid_0's rmse: 1.04047\n",
      "[1500]\tvalid_0's rmse: 1.02988\n",
      "[2000]\tvalid_0's rmse: 1.02491\n",
      "[2500]\tvalid_0's rmse: 1.02248\n",
      "[3000]\tvalid_0's rmse: 1.01878\n",
      "[3500]\tvalid_0's rmse: 1.01858\n",
      "[4000]\tvalid_0's rmse: 1.01856\n",
      "[4500]\tvalid_0's rmse: 1.01669\n",
      "[5000]\tvalid_0's rmse: 1.01506\n",
      "[5500]\tvalid_0's rmse: 1.01441\n",
      "[6000]\tvalid_0's rmse: 1.01343\n",
      "[6500]\tvalid_0's rmse: 1.01391\n",
      "Early stopping, best iteration is:\n",
      "[6033]\tvalid_0's rmse: 1.01335\n",
      "Predicting for a fold: 2 | started at | ⏰: 13:53:33\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "\n",
    "for fold, (trn_idx, val_idx) in enumerate(skf.split(train_df, train_df['meter_reading'], groups=train_df['group'])):\n",
    "    gc.collect()\n",
    "    \n",
    "    X_train, y_train = train_df[features].iloc[trn_idx], train_df['meter_reading'].iloc[trn_idx]\n",
    "    X_valid, y_valid = train_df[features].iloc[val_idx], train_df['meter_reading'].iloc[val_idx]\n",
    "\n",
    "    start = time.strftime(\"%H:%M:%S\")\n",
    "    print(f\"Starting for a fold: {fold} | ⏰: {start}\")\n",
    "    \n",
    "    lgb_train = lgb.Dataset(X_train, y_train, categorical_feature=categoricals)\n",
    "    lgb_eval = lgb.Dataset(X_valid, y_valid, categorical_feature=categoricals)\n",
    "    gbm = lgb.train(params,\n",
    "                    lgb_train,\n",
    "                    num_boost_round=10000,\n",
    "                    valid_sets=(lgb_eval),\n",
    "                    early_stopping_rounds=500,\n",
    "                    verbose_eval = 500,\n",
    "                    categorical_feature=categoricals)\n",
    "\n",
    "    predicting_started = time.strftime(\"%H:%M:%S\")\n",
    "    print(f\"Predicting for a fold: {fold} | started at | ⏰: {predicting_started}\")\n",
    "    \n",
    "    predictions['group{}'.format(fold+1)] = np.expm1(gbm.predict(test_df[features]))\n",
    "    \n",
    "    val_pred = gbm.predict(X_valid)\n",
    "    oof['predict'].iloc[val_idx] = val_pred\n",
    "    oof['meter_reading'].iloc[val_idx] = y_valid\n",
    "    \n",
    "    val_pred = np.array(val_pred).clip(min=0)\n",
    "    val_score = RMSLE(y_valid, val_pred)\n",
    "    val_rmsle.append(val_score)\n",
    "    \n",
    "    print(f\"RMSE: {val_score}\")\n",
    "\n",
    "    models.append(gbm)   \n",
    "    \n",
    "    feature_importance_extraction_started = time.strftime(\"%H:%M:%S\")\n",
    "    print(f\"Feature importance extraction a fold: {fold} | started at | ⏰: {feature_importance_extraction_started}\")\n",
    "    \n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"feature\"] = features\n",
    "    fold_importance_df[\"importance\"] = gbm.feature_importance()\n",
    "    fold_importance_df[\"fold\"] = fold + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof['predict'][oof['predict'] < 0] = 0\n",
    "mean_rmsle = np.mean(val_rmsle)\n",
    "std_rmsle = np.std(val_rmsle)\n",
    "all_rmsle = RMSLE(oof['meter_reading'], oof['predict'])\n",
    "print(\"Mean rmse: %.9f, std: %.9f. All auc: %.9f.\" % (mean_rmsle, std_rmsle, all_rmsle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = (feature_importance_df[[\"feature\", \"importance\"]]\n",
    "        .groupby(\"feature\")\n",
    "        .mean()\n",
    "        .sort_values(by=\"importance\", ascending=False)[:1000].index)\n",
    "best_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n",
    "\n",
    "plt.figure(figsize=(14,26))\n",
    "sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\n",
    "plt.title('CatBoost Features (averaged over folds)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('catboost_importances.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[predictions < 0] = 0\n",
    "predictions['target'] = np.mean(predictions[[col for col in predictions.columns if col not in ['building_id']]].values, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('./data/sample_submission.csv')\n",
    "\n",
    "predictions['target'] = np.mean(predictions[[col for col in predictions.columns if col not in ['building_id']]].values, axis=1)\n",
    "sample_submission['meter_reading'] = np.clip(predictions['target'].values, a_min=0, a_max=None)\n",
    "sample_submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (model) in enumerate(models):\n",
    "    model.save_model(f'model{i}.txt')\n",
    "\n",
    "    #bst = lgb.Booster(model_file='mode.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

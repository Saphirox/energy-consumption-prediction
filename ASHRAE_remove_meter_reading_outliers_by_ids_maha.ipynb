{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maksymsuprunenko/miniconda3/lib/python3.7/site-packages/lightgbm/__init__.py:48: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import gc\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GroupKFold\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#from catboost import Pool, CatBoostRegressor\n",
    "import lightgbm as lgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '../input/ashrae-energy-prediction/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY_NAME = 'building_id__meter__key'\n",
    "KEY = ['building_id', 'meter', 'site_id']\n",
    "\n",
    "def to_key(train_df, test_df):\n",
    "    le_key = LabelEncoder()\n",
    "    train_df[KEY_NAME] = train_df[KEY].apply(\\\n",
    "        lambda x: str(x['building_id']) + \"_\" + str(x['meter']) + \"_\" +   str(x['site_id']), axis=1)\n",
    "    \n",
    "    test_df[KEY_NAME] = test_df[KEY].apply(\\\n",
    "        lambda x: str(x['building_id']) + \"_\" + str(x['meter']) + \"_\" +   str(x['site_id']), axis=1)\n",
    "    \n",
    "    le_key.fit(train_df[KEY_NAME])\n",
    "    train_df[KEY_NAME] = le_key.transform(train_df[KEY_NAME])\n",
    "    train_df[KEY_NAME] = train_df[KEY_NAME].astype(np.int32)\n",
    "    \n",
    "    test_df[KEY_NAME] = le_key.transform(test_df[KEY_NAME])\n",
    "    test_df[KEY_NAME] = test_df[KEY_NAME].astype(np.int32)\n",
    "    \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASHRAE3Preprocessor(object):\n",
    "       \n",
    "    @classmethod\n",
    "    def reduce_memory_usage(cls, df):\n",
    "        numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "        for col in df.columns:\n",
    "            col_type = df[col].dtypes\n",
    "            if col_type in numerics:\n",
    "                c_min = df[col].min()\n",
    "                c_max = df[col].max()\n",
    "                if str(col_type)[:3] == 'int':\n",
    "                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                        df[col] = df[col].astype(np.int8)\n",
    "                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                        df[col] = df[col].astype(np.int16)\n",
    "                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                        df[col] = df[col].astype(np.int32)\n",
    "                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                        df[col] = df[col].astype(np.int64)  \n",
    "                else:\n",
    "                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                        df[col] = df[col].astype(np.float16)\n",
    "                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                        df[col] = df[col].astype(np.float32)\n",
    "                    else:\n",
    "                        df[col] = df[col].astype(np.float64)    \n",
    "\n",
    "        return df\n",
    "  \n",
    "    @classmethod\n",
    "    def timestamp_align(cls, weather_df):\n",
    "        weather_key = ['site_id', 'timestamp']\n",
    "\n",
    "        temp_skeleton = weather_df[weather_key + ['air_temperature']].sort_values(by=weather_key).copy()\n",
    "\n",
    "        # calculate ranks of hourly temperatures within date/site_id chunks\n",
    "        temp_skeleton['temp_rank'] = temp_skeleton.groupby(['site_id', temp_skeleton.timestamp.dt.date])['air_temperature'].rank('average')\n",
    "\n",
    "        # create a dataframe of site_ids (0-16) x mean hour rank of temperature within day (0-23)\n",
    "        df_2d = temp_skeleton.groupby(['site_id', temp_skeleton.timestamp.dt.hour])['temp_rank'].mean().unstack(level=1)\n",
    "\n",
    "        # Subtract the columnID of temperature peak by 14, getting the timestamp alignment gap.\n",
    "        site_ids_offsets = pd.Series(df_2d.values.argmax(axis=1) - 14)\n",
    "        site_ids_offsets.index.name = 'site_id'\n",
    "        \n",
    "        weather_df['offset'] = weather_df.site_id.map(site_ids_offsets)\n",
    "        weather_df['timestamp'] = (weather_df['timestamp'] - pd.to_timedelta(weather_df['offset'], unit='H'))\n",
    "        \n",
    "        del temp_skeleton, df_2d, site_ids_offsets\n",
    "        \n",
    "        return weather_df\n",
    "        \n",
    "    @classmethod\n",
    "    def load_dataset(cls, df_path, weather_path, metadata_path):\n",
    "        metadata = pd.read_csv(metadata_path)\n",
    "        \n",
    "        base_df = pd.read_csv(df_path, parse_dates=['timestamp'])\n",
    "        merged_df = pd.merge(base_df, metadata, how=\"left\", on=[\"building_id\"])\n",
    "\n",
    "        weather_df = pd.read_csv(weather_path, parse_dates=['timestamp'])\n",
    "        weather_df = cls.timestamp_align(weather_df)\n",
    "        \n",
    "        df = pd.merge(merged_df, weather_df, how=\"left\", on=[\"site_id\", \"timestamp\"])\n",
    "        \n",
    "        del metadata, base_df, merged_df, weather_df\n",
    "        \n",
    "        # drop all NaN rows which are generated by timestamp alignment\n",
    "        if(cls.train==True):\n",
    "            df = df.loc[~(df['air_temperature'].isnull() & df['cloud_coverage'].isnull() & df['dew_temperature'].isnull() & df['precip_depth_1_hr'].isnull() & \\\n",
    "                          df['sea_level_pressure'].isnull() & df['wind_direction'].isnull() & df['wind_speed'].isnull() & df['offset'].isnull())]\n",
    "            \n",
    "            df = df.query('not (site_id == 0 & meter == 0 & timestamp <= \"2016-05-20\") and not (meter == 2 & building_id == 1099)')\n",
    "        \n",
    "        df['offset'] = np.uint8(df['offset'])\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    @classmethod\n",
    "    def fit(cls, df):\n",
    "        data_ratios =  df.count()/len(df)\n",
    "        cls.avgs = df.loc[:, data_ratios < 1.0].mean()\n",
    "        cls.pu_le = LabelEncoder()\n",
    "        cls.pu_le.fit(df[\"primary_use\"])\n",
    "     \n",
    "    \n",
    "    @classmethod\n",
    "    def average_imputation(cls, df, column_name):\n",
    "        imputation = df.groupby(['timestamp'])[column_name].mean()\n",
    "    \n",
    "        df.loc[df[column_name].isnull(), column_name] = df[df[column_name].isnull()][[column_name]].apply(lambda x: imputation[df['timestamp'][x.index]].values)\n",
    "        \n",
    "        del imputation\n",
    "        return df\n",
    "\n",
    "    @classmethod\n",
    "    def transform(cls, df):\n",
    "        # refill NAN with averages\n",
    "        columns_with_nan = cls.avgs.index.values\n",
    "        for i in range(len(columns_with_nan)):\n",
    "            df = cls.average_imputation(df, columns_with_nan[i])\n",
    "        \n",
    "        df['primary_use'] = np.uint8(cls.pu_le.transform(df['primary_use']))  # encode labels\n",
    "\n",
    "             \n",
    "        # expand datetime into its components\n",
    "        df['hour'] = np.uint8(df['timestamp'].dt.hour)\n",
    "        df['day'] = np.uint8(df['timestamp'].dt.day)\n",
    "        df['weekday'] = np.uint8(df['timestamp'].dt.weekday)\n",
    "        df['dayofweek'] = np.uint8(df['timestamp'].dt.dayofweek)\n",
    "        df['month'] = np.uint8(df['timestamp'].dt.month)\n",
    "        df['year'] = np.uint8(df['timestamp'].dt.year-2000)\n",
    "        \n",
    "        df['dayofyear'] = df['timestamp'].dt.dayofyear.astype(np.int16)\n",
    "        df['weekofyear'] = df['timestamp'].dt.weekofyear.astype(np.int8)\n",
    "        df['week_month_datetime'] = df['timestamp'].dt.day/7\n",
    "        \n",
    "        # parse and cast columns to a smaller type\n",
    "        df.rename(columns={\"square_feet\": \"log_square_feet\"}, inplace=True)\n",
    "        df['log_square_feet'] = np.float16(np.log(df['log_square_feet']))\n",
    "        df['year_built'] = np.uint8(df['year_built']-1900)\n",
    "        df['floor_count'] = np.uint8(df['floor_count'])\n",
    "        df['wind_direction'] = np.uint16(df['wind_direction'])\n",
    "    \n",
    "        # extract target column\n",
    "        if 'meter_reading' in df.columns:\n",
    "            df['meter_reading'] = np.log1p(df['meter_reading']).astype(np.float32) # comp metric uses log errors \n",
    "            \n",
    "        return df\n",
    "    \n",
    "    @classmethod\n",
    "    def add_features(cls, df):\n",
    "        beaufort_boundaries = [0, 0.3, 1.6, 3.4, 5.5, 8, 10.8, 13.9, 17.2, 20.8, 24.5, 28.5, 33, 10000]\n",
    "        df['wind_speed_beaufort_scale'] = pd.cut(\n",
    "            x=df['wind_speed'], \n",
    "            bins=beaufort_boundaries, \n",
    "            labels=range(len(beaufort_boundaries)-1),\n",
    "            right=False,\n",
    "        ).cat.codes\n",
    "        df['wind_speed_beaufort_scale'] = np.uint8(df['wind_speed_beaufort_scale'])\n",
    "        \n",
    "        #df.loc[(df['primary_use'] == le.transform(['Education'])[0]) & (df['month'] >= 6) & (df['month'] <= 8), 'is_vacation_month'] = np.int8(1)\n",
    "        #df.loc[df['is_vacation_month']!=1, 'is_vacation_month'] = np.int8(0)\n",
    "        \n",
    "        df['group'] = df['month']\n",
    "        df['group'].replace((6, 7, 8), 21, inplace=True)\n",
    "        df['group'].replace((9, 10, 11), 22, inplace=True)\n",
    "        df['group'].replace((3, 4, 5), 23, inplace=True)\n",
    "        df['group'].replace((1, 2, 12), 24, inplace=True)\n",
    "        df['group'].replace((21), 1, inplace=True)\n",
    "        df['group'].replace((22), 2, inplace=True)\n",
    "        df['group'].replace((23), 3, inplace=True)\n",
    "        df['group'].replace((24), 4, inplace=True)\n",
    "        df['group'] = np.uint8(df['group'])\n",
    "        \n",
    "        \n",
    "        # remove redundant columns\n",
    "        for col in df.columns:\n",
    "            if col in ['timestamp', 'row_id']:\n",
    "                del df[col]\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    @classmethod\n",
    "    def get_prepared_dataset(cls, df_path, weather_path, metadata, train):\n",
    "        cls.train = train\n",
    "        \n",
    "        df = cls.load_dataset(df_path, weather_path, metadata)\n",
    "        \n",
    "        start_mem = df.memory_usage().sum() / 1024**2 \n",
    "      \n",
    "        df = cls.reduce_memory_usage(df)\n",
    "        \n",
    "        end_mem = df.memory_usage().sum() / 1024**2\n",
    "        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem)) \n",
    "        \n",
    "        cls.fit(df)\n",
    "        df = cls.transform(df)\n",
    "        df = cls.add_features(df)\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = root + \"train.csv\"\n",
    "test_path = root + \"test.csv\"\n",
    "    \n",
    "train_weather_path = root + \"weather_train.csv\"\n",
    "test_weather_path = root + \"weather_test.csv\"\n",
    "\n",
    "metadata_path = root + 'building_metadata.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(df_path, df_weather_path, metadata_path, train):\n",
    "    return ASHRAE3Preprocessor.get_prepared_dataset(df_path, df_weather_path, metadata_path, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:74: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 1036.33 Mb (59.9% reduction)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/numpy/core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/opt/conda/lib/python3.6/site-packages/numpy/core/_methods.py:36: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 2187.13 Mb (59.9% reduction)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = load_dataset(train_path, train_weather_path, metadata_path, train =True)\n",
    "gc.collect()\n",
    "\n",
    "test_df = load_dataset(test_path, test_weather_path, metadata_path, train =False)\n",
    "gc.collect()\n",
    "\n",
    "train_df, test_df = to_key(train_df, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSLE(y_true, y_pred, *args, **kwargs):\n",
    "    return np.sqrt(mean_squared_log_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals = [\"site_id\", \"building_id\", \"primary_use\", \"hour\", \"weekday\", 'dayofweek', \"meter\", \"wind_direction\", 'group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "            'boosting_type': 'gbdt',\n",
    "            'objective': 'regression',\n",
    "            'metric': {'rmse'},\n",
    "            'subsample': 0.4,\n",
    "            'learning_rate': 0.15,\n",
    "            'num_leaves': 40,\n",
    "            'feature_fraction': 0.4,\n",
    "            'lambda_l1': 1,  \n",
    "            'lambda_l2': 1\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.[train_df.groupby()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "skf = GroupKFold(n_splits=4)\n",
    "oof = train_df[['meter_reading']]\n",
    "oof['predict'] = 0\n",
    "predictions =  pd.DataFrame()\n",
    "val_rmsle = []\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "features = [col for col in train_df.columns if col not in ['meter_reading', 'year', 'month', 'day', 'timestamp']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.covariance import EmpiricalCovariance, MinCovDet\n",
    "\n",
    "def remove_outliers(train_df):\n",
    "    def calc_q(train_df):\n",
    "        item_price = train_df\n",
    "        i = 0.95\n",
    "        q = np.quantile(item_price, (1-i, i))\n",
    "        return q\n",
    "    \n",
    "    def calc_mahalanobis(x):\n",
    "        robust_cov = MinCovDet(random_state=42).fit(x)\n",
    "        maha = robust_cov.mahalanobis(x)\n",
    "        return maha\n",
    "        \n",
    "\n",
    "    def func(x):\n",
    "        \n",
    "        temp = x[['air_temperature', 'cloud_coverage', 'dew_temperature', \\\n",
    "       'precip_depth_1_hr', 'sea_level_pressure', 'wind_direction', \\\n",
    "       'wind_speed']]\n",
    "        \n",
    "        temp = temp.notna()\n",
    "        \n",
    "        for i in temp.columns:\n",
    "            temp[i] = temp[i].interpolate()\n",
    "            \n",
    "        var = calc_mahalanobis(temp)\n",
    "        \n",
    "        q = calc_q(var)\n",
    "        re = x[(var >= q[0]) & (var <= q[1])] \n",
    "        \n",
    "        var = calc_mahalanobis(x[['meter_reading']])\n",
    "        \n",
    "        q = calc_q(var)\n",
    "        re = re[(var >= q[0]) & (var <= q[1])] \n",
    "        \n",
    "        return re\n",
    "\n",
    "    train_df = train_df.groupby(KEY, as_index=False).apply(func)\n",
    "    \n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting for a fold: 0 | ⏰: 18:46:18\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[100]\tvalid_0's rmse: 0.795788\n",
      "[200]\tvalid_0's rmse: 0.751304\n",
      "[300]\tvalid_0's rmse: 0.72575\n",
      "[400]\tvalid_0's rmse: 0.71637\n",
      "[500]\tvalid_0's rmse: 0.707047\n",
      "[600]\tvalid_0's rmse: 0.70297\n",
      "[700]\tvalid_0's rmse: 0.699847\n",
      "[800]\tvalid_0's rmse: 0.697058\n",
      "[900]\tvalid_0's rmse: 0.694065\n",
      "[1000]\tvalid_0's rmse: 0.692284\n",
      "[1100]\tvalid_0's rmse: 0.690008\n",
      "[1200]\tvalid_0's rmse: 0.688362\n",
      "[1300]\tvalid_0's rmse: 0.687282\n",
      "[1400]\tvalid_0's rmse: 0.685642\n",
      "[1500]\tvalid_0's rmse: 0.683452\n",
      "[1600]\tvalid_0's rmse: 0.681956\n",
      "[1700]\tvalid_0's rmse: 0.680812\n",
      "[1800]\tvalid_0's rmse: 0.680539\n",
      "[1900]\tvalid_0's rmse: 0.679437\n",
      "[2000]\tvalid_0's rmse: 0.678548\n",
      "[2100]\tvalid_0's rmse: 0.677786\n",
      "[2200]\tvalid_0's rmse: 0.676535\n",
      "[2300]\tvalid_0's rmse: 0.676183\n",
      "[2400]\tvalid_0's rmse: 0.675844\n",
      "[2500]\tvalid_0's rmse: 0.674128\n",
      "[2600]\tvalid_0's rmse: 0.672342\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "\n",
    "for fold, (trn_idx, val_idx) in enumerate(skf.split(train_df, train_df['meter_reading'], groups=train_df['group'])):\n",
    "    gc.collect()\n",
    "    \n",
    "    train = train_df.iloc[trn_idx]\n",
    "    train = remove_outliers(train)\n",
    "    \n",
    "    X_train, y_train = train[features], train['meter_reading']\n",
    "    X_valid, y_valid = train_df[features].iloc[val_idx], train_df['meter_reading'].iloc[val_idx]\n",
    "    \n",
    "    start = time.strftime(\"%H:%M:%S\")\n",
    "    print(f\"Starting for a fold: {fold} | ⏰: {start}\")\n",
    "    \n",
    "    q = calc_q(y_train)\n",
    "    X_train = X_train[(y_train >= q[0]) & (y_train <= q[1])]\n",
    "    y_train = y_train[(y_train >= q[0]) & (y_train <= q[1])]\n",
    "     \n",
    "    \n",
    "    lgb_train = lgb.Dataset(X_train, y_train, categorical_feature=categoricals)\n",
    "    lgb_eval = lgb.Dataset(X_valid, y_valid, categorical_feature=categoricals)\n",
    "    gbm = lgb.train(params,\n",
    "                    lgb_train,\n",
    "                    num_boost_round=10000,\n",
    "                    valid_sets=(lgb_eval),\n",
    "                    early_stopping_rounds=500,\n",
    "                    verbose_eval = 100,\n",
    "                    categorical_feature=categoricals)\n",
    "\n",
    "    predicting_started = time.strftime(\"%H:%M:%S\")\n",
    "    print(f\"Predicting for a fold: {fold} | started at | ⏰: {predicting_started}\")\n",
    "    \n",
    "    predictions['group{}'.format(fold+1)] = np.expm1(gbm.predict(test_df[features]))\n",
    "    \n",
    "    val_pred = gbm.predict(X_valid)\n",
    "    oof['predict'].iloc[val_idx] = val_pred\n",
    "    oof['meter_reading'].iloc[val_idx] = y_valid\n",
    "    \n",
    "    val_pred = np.array(val_pred).clip(min=0)\n",
    "    val_score = RMSLE(y_valid, val_pred)\n",
    "    val_rmsle.append(val_score)\n",
    "    \n",
    "    print(f\"RMSE: {val_score}\")\n",
    "\n",
    "    models.append(gbm)   \n",
    "    \n",
    "    feature_importance_extraction_started = time.strftime(\"%H:%M:%S\")\n",
    "    print(f\"Feature importance extraction a fold: {fold} | started at | ⏰: {feature_importance_extraction_started}\")\n",
    "    \n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"feature\"] = features\n",
    "    fold_importance_df[\"importance\"] = gbm.feature_importance()\n",
    "    fold_importance_df[\"fold\"] = fold + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof['predict'][oof['predict'] < 0] = 0\n",
    "mean_rmsle = np.mean(val_rmsle)\n",
    "std_rmsle = np.std(val_rmsle)\n",
    "all_rmsle = RMSLE(oof['meter_reading'], oof['predict'])\n",
    "print(\"Mean rmse: %.9f, std: %.9f. All auc: %.9f.\" % (mean_rmsle, std_rmsle, all_rmsle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = (feature_importance_df[[\"feature\", \"importance\"]]\n",
    "        .groupby(\"feature\")\n",
    "        .mean()\n",
    "        .sort_values(by=\"importance\", ascending=False)[:1000].index)\n",
    "best_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n",
    "\n",
    "plt.figure(figsize=(14,26))\n",
    "sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\n",
    "plt.title('CatBoost Features (averaged over folds)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('catboost_importances.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[predictions < 0] = 0\n",
    "predictions['target'] = np.mean(predictions[[col for col in predictions.columns if col not in ['building_id']]].values, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('./data/sample_submission.csv')\n",
    "\n",
    "predictions['target'] = np.mean(predictions[[col for col in predictions.columns if col not in ['building_id']]].values, axis=1)\n",
    "sample_submission['meter_reading'] = np.clip(predictions['target'].values, a_min=0, a_max=None)\n",
    "sample_submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = lgb.plot_metric(models[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = lgb.plot_metric(models[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = lgb.plot_metric(models[2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = lgb.plot_metric(models[3])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

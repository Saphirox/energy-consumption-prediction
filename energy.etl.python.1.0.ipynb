{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## <center> ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 289.19 Mb (53.1% reduction)\n",
      "Mem. usage decreased to 596.49 Mb (53.1% reduction)\n",
      "Mem. usage decreased to  3.07 Mb (68.1% reduction)\n",
      "Mem. usage decreased to  6.08 Mb (68.1% reduction)\n",
      "Mem. usage decreased to  0.03 Mb (60.3% reduction)\n",
      "Wall time: 1min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "root = 'data/'\n",
    "train_df = pd.read_csv(root + 'train.csv')\n",
    "train_df[\"timestamp\"] = pd.to_datetime(train_df[\"timestamp\"], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "weather_train_df = pd.read_csv(root + 'weather_train.csv')\n",
    "test_df = pd.read_csv(root + 'test.csv')\n",
    "weather_test_df = pd.read_csv(root + 'weather_test.csv')\n",
    "building_meta_df = pd.read_csv(root + 'building_metadata.csv')\n",
    "sample_submission = pd.read_csv(root + 'sample_submission.csv')\n",
    "\n",
    "## Function to reduce the DF size\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "train_df = reduce_mem_usage(train_df)\n",
    "test_df = reduce_mem_usage(test_df)\n",
    "\n",
    "weather_train_df = reduce_mem_usage(weather_train_df)\n",
    "weather_test_df = reduce_mem_usage(weather_test_df)\n",
    "building_meta_df = reduce_mem_usage(building_meta_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def timestamp_align(weather_df):\n",
    "    weather_key = ['site_id', 'timestamp']\n",
    "\n",
    "    temp_skeleton = weather_df[weather_key + ['air_temperature']].sort_values(by=weather_key).copy()\n",
    "\n",
    "    # calculate ranks of hourly temperatures within date/site_id chunks\n",
    "    temp_skeleton['temp_rank'] = temp_skeleton.groupby(['site_id', temp_skeleton.timestamp.dt.date])['air_temperature'].rank('average')\n",
    "\n",
    "    # create a dataframe of site_ids (0-16) x mean hour rank of temperature within day (0-23)\n",
    "    df_2d = temp_skeleton.groupby(['site_id', temp_skeleton.timestamp.dt.hour])['temp_rank'].mean().unstack(level=1)\n",
    "\n",
    "    # Subtract the columnID of temperature peak by 14, getting the timestamp alignment gap.\n",
    "    site_ids_offsets = pd.Series(df_2d.values.argmax(axis=1) - 14)\n",
    "    site_ids_offsets.index.name = 'site_id'\n",
    "\n",
    "    weather_df['offset'] = weather_df.site_id.map(site_ids_offsets)\n",
    "    weather_df['timestamp'] = (weather_df['timestamp'] - pd.to_timedelta(weather_df['offset'], unit='H'))\n",
    "    \n",
    "    add_lag_feature(weather_df)\n",
    "    \n",
    "    del temp_skeleton, df_2d, site_ids_offsets\n",
    "\n",
    "    return weather_df\n",
    "        \n",
    "\n",
    "train_df['timestamp'] = pd.to_datetime(train_df['timestamp'])\n",
    "test_df['timestamp'] = pd.to_datetime(test_df['timestamp'])\n",
    "weather_train_df['timestamp'] = pd.to_datetime(weather_train_df['timestamp'])\n",
    "weather_train_df = timestamp_align(weather_train_df)\n",
    "weather_test_df['timestamp'] = pd.to_datetime(weather_test_df['timestamp'])\n",
    "weather_test_df = timestamp_align(weather_test_df)\n",
    "\n",
    "building_meta_df['primary_use'] = building_meta_df['primary_use'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "temp_df = train_df[['building_id']]\n",
    "temp_df = temp_df.merge(building_meta_df, on=['building_id'], how='left')\n",
    "del temp_df['building_id']\n",
    "train_df = pd.concat([train_df, temp_df], axis=1)\n",
    "\n",
    "temp_df = test_df[['building_id']]\n",
    "temp_df = temp_df.merge(building_meta_df, on=['building_id'], how='left')\n",
    "\n",
    "del temp_df['building_id']\n",
    "test_df = pd.concat([test_df, temp_df], axis=1)\n",
    "del temp_df, building_meta_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = train_df[['site_id','timestamp']]\n",
    "temp_df = temp_df.merge(weather_train_df, on=['site_id','timestamp'], how='left')\n",
    "\n",
    "del temp_df['site_id'], temp_df['timestamp']\n",
    "train_df = pd.concat([train_df, temp_df], axis=1)\n",
    "\n",
    "temp_df = test_df[['site_id','timestamp']]\n",
    "temp_df = temp_df.merge(weather_test_df, on=['site_id','timestamp'], how='left')\n",
    "\n",
    "del temp_df['site_id'], temp_df['timestamp']\n",
    "test_df = pd.concat([test_df, temp_df], axis=1)\n",
    "\n",
    "del temp_df, weather_train_df, weather_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_pickle(root+'train_df.pkl')\n",
    "test_df.to_pickle(root+'test_df.pkl')\n",
    "   \n",
    "del train_df, test_df\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
